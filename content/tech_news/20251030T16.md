---
title: "2025.10.30.16 过去4小时全球AI发生了什么？"
date: 2025-10-30T16:00:14+08:00
categories: ["AI 行业动态", "大模型研究", "自动驾驶"]
---

1.  [李想3小时长谈：AI的终局与理想的AGI之路-宝玉](https://x.com/dotey/status/1983768925289509288)

    理想汽车CEO李想在一次深度对话中分享了对AI的独到见解及理想汽车的未来战略。他强调，AI的真正价值在于从**信息工具**升级为“知行合一”的**生产工具**。李想对DeepSeek坚持“反人性”的**最佳实践**表示高度赞赏，并透露其开源成果加速了理想VLA（视觉-语言-行动）模型的研发进程。在自动驾驶领域，他详细阐述了理想如何通过“预训练”、“后训练”和“强化训练”三步法，打造具备“人类智能”的VLA司机大模型。李想还宣布理想汽车的目标是到2030年成为全球领先的**人工智能终端企业**，将汽车视为首个“AGI终端”，并计划向家庭、穿戴设备等其他AGI终端拓展。他认为在AI时代，人类应专注于提升**智慧**与**关系**。

2.  [AI编码智能体：追求速度优化的务实战略-宝玉](https://x.com/dotey/status/1983745841975218281)

    关于Cursor和Windsurf等AI编码智能体公司为何选择发布**速度优化模型**而非从头构建基础模型，Cline作者分析指出，这是一种高效且务实的战略。通过对现有**开源大模型**（如Qwen3）进行**强化学习微调**，并部署在Cerebras等优化硬件上，公司能以较小的资源投入，实现智能与速度的**帕累托前沿**。这种方法规避了从零开始构建基础模型所需的巨额资金、漫长周期和高昂风险。对于AI编码智能体而言，其核心价值在于在现有基础上进行精细化微调和高效推理优化，以快速响应市场需求，尽管不同应用场景对智能和速度的需求会有所侧重。

3.  [新参数高效微调（PEFT）方法实现SOTA并减少GPU内存-宝玉](https://x.com/dotey/status/1983746064164515948)

    一项新的研究提出了一种创新的**参数高效微调（PEFT）**方法，在模型训练过程中仅需调整0.02%的参数，便能达到**新的SOTA（State-of-the-Art）**性能。该方法相较于传统的selection-based微调技术，显著**减少了GPU内存占用**。这一技术突破对于在有限计算资源下高效部署和优化大型AI模型具有重要意义，有望降低相关研究和应用的门槛。